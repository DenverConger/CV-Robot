{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text_Generation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMFmQOGaik3CVUQnfB6sTGc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PNh4aPtPY87I"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n","import numpy as np\n","import os\n","import time\n","physical_devices = tf.config.list_physical_devices('GPU')\n","try:\n","  tf.config.set_logical_device_configuration(\n","    physical_devices[0],\n","    tf.config.LogicalDeviceConfiguration(memory_limit=4096))\n","\n","  logical_devices = tf.config.list_logical_devices('GPU')\n","  assert len(logical_devices) == len(physical_devices) + 1\n","\n","  tf.config.set_logical_device_configuration(\n","    physical_devices[0],\n","    tf.config.LogicalDeviceConfiguration(memory_limit=4096))\n","except:\n","#   # Invalid device or cannot modify logical devices once initialized.\n","  pass\n","path_to_file = tf.keras.utils.get_file('austen.txt', 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt')\n","# path_to_file = ('D:\\Coding\\Eye-of-the-World_-The-Robert-Jordan.txt')\n","\n","# path_to_file = ('D:\\Coding\\wotCombined.txt')\n","\n","\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","# print('Length of text: {} characters'.format(len(text)))\n","# print(text[:200])\n","# Now we'll get a list of the unique characters in the file. This will form the\n","# vocabulary of our network. There may be some characters we want to remove from this \n","# set as we refine the network.\n","vocab = sorted(set(text))\n","print('{} unique characters'.format(len(vocab)))\n","ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n","chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n","\n","# Here's a little helper function that we can use to turn a sequence of ids\n","# back into a string:\n","# turn them into a string:\n","def text_from_ids(ids):\n","  joinedTensor = tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n","  return joinedTensor.numpy().decode(\"utf-8\")\n","all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n","\n","ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n","seq_length = 100\n","sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","# This function will generate our sequence pairs:\n","def split_input_target(sequence):\n","    input_text = sequence[:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text\n","\n","# Call the function for every sequence in our list to create a new dataset\n","# of input->target pairs\n","dataset = sequences.map(split_input_target)\n","\n","# Finally, we'll randomize the sequences so that we don't just memorize the books\n","# in the order they were written, then build a new streaming dataset from that.\n","# Using a streaming dataset allows us to pass the data to our network bit by bit,\n","# rather than keeping it all in memory. We'll set it to figure out how much data\n","# to prefetch in the background.\n","\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","dataset = (\n","    dataset\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE, drop_remainder=True)\n","    .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","# Create our custom model. Given a sequence of characters, this\n","# model's job is to predict what character should come next.\n","class AustenTextModel(tf.keras.Model):\n","\n","  # This is our class constructor method, it will be executed when\n","  # we first create an instance of the class \n","  def __init__(self,vocab_size, embedding_dim, rnn_units,name = None):\n","    super(AustenTextModel, self).__init__(name=name)\n","    # Our model will have three layers:\n","    \n","    # 1. An embedding layer that handles the encoding of our vocabulary into\n","    #    a vector of values suitable for a neural network\n","\n","    \n","\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","\n","    # 2. A GRU layer that handles the \"memory\" aspects of our RNN. If you're\n","    #    wondering why we use GRU instead of LSTM, and whether LSTM is better,\n","    #    take a look at this article: https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm\n","    #    then consider trying out LSTM instead (or in addition to!)\n","    # self.gru = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n","    self.lstm = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n","    # self.dropouta = tf.keras.layers.Dropout(0.4)\n","    # self.lstmb = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n","    # self.dropoutb = tf.keras.layers.Dropout(0.4)\n","    # self.lstmc = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n","    # self.dropoutc = tf.keras.layers.Dropout(0.4)\n","    # self.lstmd = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n","\n","    # 3. Our output layer that will give us a set of probabilities for each\n","    #    character in our vocabulary.\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","  # This function will be executed for each epoch of our training. Here\n","  # we will manually feed information from one layer of our network to the \n","  # next.\n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","    print('inputs.shape')\n","    print(inputs.shape)\n","    # 1. Feed the inputs into the embedding layer, and tell it if we are\n","    #    training or predicting\n","    x = self.embedding(x, training=training)\n","\n","    # 2. If we don't have any state in memory yet, get the initial random state\n","    #    from our GRUI layer.\n","    if states is None:\n","      states = self.lstm.get_initial_state(x)\n","    \n","    # 3. Now, feed the vectorized input along with the current state of memory\n","    #    into the gru layer.\n","    x, state_h, states_l = self.lstm(x, initial_state=states, training=training)\n","    # states = (state_h, states_l)\n","    # state_h = states[0]\n","    # states_l = states[1]\n","    # x, state_h, states_l = self.lstmb(x, initial_state=states, training=training)\n","    states = (state_h, states_l)\n","    # state_h = states[0]\n","    # states_l = states[1]\n","    # x, state_h, states_l = self.lstmc(x, initial_state=states, training=training)\n","    \n","\n","    # states = (state_h, states_l)\n","    # 4. Finally, pass the results on to the dense layer\n","    x = self.dense(x, training=training)\n","\n","    # 5. Return the results\n","    if return_state:\n","      return x, states\n","    else: \n","      return x\n","\n","\n","\n","  # def get_config(self):\n","  #       config = super(AustenTextModel, self).get_config()\n","  #       config.update({\n","  #           'vocab_size': self.vocab_size,\n","  #           'embedding_dim': self.embedding_dim,\n","  #           'rnn_units': self.rnn_units\n","  #       })\n","  #       return config\n","\n","vocab_size=len(ids_from_chars.get_vocabulary())\n","embedding_dim = 256\n","rnn_units = 2048\n","\n","model = AustenTextModel(vocab_size, embedding_dim, rnn_units,name = 'austen_text_model')\n","\n","# Verify the output of our model is correct by running one sample through\n","# This will also compile the model for us. This step will take a bit.\n","for input_example_batch, target_example_batch in  dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n","model.compile(optimizer='adam', loss=loss)\n","\n","history = model.fit(dataset, epochs=25)\n","\n","class OneStep(tf.keras.Model):\n","  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n","    super().__init__()\n","    self.temperature=temperature\n","    self.model = model\n","    self.chars_from_ids = chars_from_ids\n","    self.ids_from_chars = ids_from_chars\n","\n","    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n","    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n","    sparse_mask = tf.SparseTensor(\n","        # Put a -inf at each bad index.\n","        values=[-float('inf')]*len(skip_ids),\n","        indices = skip_ids,\n","        # Match the shape to the vocabulary\n","        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n","    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n","\n","  @tf.function\n","  def generate_one_step(self, inputs, states=None):\n","    # Convert strings to token IDs.\n","    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","    input_ids = self.ids_from_chars(input_chars).to_tensor()\n","    print('inputs2.shape')\n","    print(inputs.shape)\n","    # Run the model.\n","    # predicted_logits.shape is [batch, char, next_char_logits] \n","    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n","                                          return_state=True)\n","    # Only use the last prediction.\n","    predicted_logits = predicted_logits[:, -1, :]\n","    predicted_logits = predicted_logits/self.temperature\n","    \n","    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n","    predicted_logits = predicted_logits + self.prediction_mask\n","\n","    # Sample the output logits to generate token IDs.\n","    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","\n","    # Return the characters and model state.\n","    return chars_from_ids(predicted_ids), states\n","one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n","\n","# Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n","# as its starting text\n","states = None\n","next_char = tf.constant(['The world seemed like such a peaceful place until the magic tree was discovered in London.'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","\n","# Print the results formatted.\n","print(result[0].numpy().decode('utf-8'))\n","# print(vocab_size)\n","# print(embedding_dim)\n","# inputs = (1,None)\n","# model.build(inputs)\n","# model.save('D:\\Coding\\ML\\RNN\\\\1_epoch_LSTM_model',save_format=\"tf\")\n","\n","# print('before')\n","# new_model = tf.keras.models.load_model('D:\\Coding\\ML\\RNN\\\\1_epoch_LSTM_model', custom_objects={'austen_text_model': AustenTextModel})\n","# # from tensorflow.keras.layers import change_model\n","# for input_example_batch, target_example_batch in dataset.take(1):\n","#     example_batch_predictions = model(input_example_batch)\n","#     print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","# loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n","# new_model.compile(optimizer='adam', loss=loss)\n","\n","# history = new_model.fit(dataset, epochs=110)\n","# # new_model = change_model(new_model,new_input_shape=(1,None))\n","# print('after')\n","# # new_model.compile(optimizer='adam', loss=loss)\n","# # history = new_model.fit(dataset, epochs=1, steps_per_epoch = 5)\n","# # Load the state of the old model\n","# print(\"after after\")\n","# one_step_model = OneStep(new_model, chars_from_ids, ids_from_chars)\n","# # Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n","# # as its starting text\n","\n","# states = None\n","# next_char = tf.constant(['Chapter 1'])\n","# result = [next_char]\n","\n","# for n in range(1000):\n","#   next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","#   result.append(next_char)\n","\n","# result = tf.strings.join(result)"]}]}